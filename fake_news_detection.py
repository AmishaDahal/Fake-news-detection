# -*- coding: utf-8 -*-
"""Fake_news_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Uj21CTE0iMjqfY8gCH0uUS_Z-s44GXXS
"""

!unzip /content/FakeNewsContent.zip

!unzip /content/RealNewsContent.zip

import os
import json
import re
import pandas as pd
import numpy as np

# Function to load news_id mapping from news.txt
def load_news_id_map(file_path):
    news_id_map = {}
    with open(file_path, 'r') as file:
        for index, line in enumerate(file, start=1):
            news_id = line.strip()  # Remove any extra whitespace/newlines
            news_id_map[news_id] = index
    return news_id_map

# Function to extract specified fields from a JSON file
def extract_fields(json_file_path, fields):
    with open(json_file_path, 'r') as f:
        data = json.load(f)
    return {field: data.get(field, None) for field in fields}  # Return None if field is missing

"""###**Loading the news data and counting the number of quotations in the news.**"""

# Function to count quotes (handles both straight and curly quotes)
def count_quotes(text):
    # Regular expression to find both straight and curly quotes
    quoted_texts = re.findall(r'[“"]([^”"]*)[”"]', text)
    return len(quoted_texts)

# Main function to process folders and map content to news IDs using news_id_map, with labels from folder prefixes
def map_news_content(news_id_map, folders, fields):
    news_data = []

    for label, folder in folders.items():
        for filename in os.listdir(folder):
            # Remove "-Webpage.json" from filename to get the base news ID
            news_id = filename.replace('-Webpage.json', '')

            # Check if this news_id exists in news_id_map
            if news_id in news_id_map:
                # Extract specified fields from JSON file
                file_path = os.path.join(folder, filename)
                content = extract_fields(file_path, fields)

                with open(file_path, 'r') as f:
                    json_data = json.load(f)
                text_content = json_data.get('text', '')

                # Count quotes in the text content
                quote_count = count_quotes(text_content)

                # Add the news_id, index, and label to the content dictionary
                content['News'] = news_id
                content['News_ID'] = news_id_map[news_id]  # Retrieve index from news_id_map
                content['Label'] = label  # Use the label key from the folders dictionary
                content['Quote_Count'] = quote_count  # Add the quote count to the content dictionary

                # Append content to news_data list
                news_data.append(content)

    # Create DataFrame from news_data
    news_df = pd.DataFrame(news_data)

    # Reorder columns to have 'News_ID' as the first column
    columns_order = ['News_ID'] + [col for col in news_df.columns if col != 'News_ID']
    news_df = news_df[columns_order].sort_values(by='News_ID').reset_index(drop=True)

    return news_df

# Example usage
news_path = "/content/News.txt"  # Path to news.txt
fakenews_folder = '/content/FakeNewsContent'  # Path to fake news content
realnews_folder = '/content/RealNewsContent'  # Path to real news content

# Load the news_id_map
news_id_map = load_news_id_map(news_path)

# Define folders with labels as keys
folders = {
    "fake": fakenews_folder,
    "real": realnews_folder
}

fields = ['title', 'text']  # Specify fields to extract

# Call the function and get the DataFrame
df = map_news_content(news_id_map, folders, fields)

df

"""###**Finding the legnth of headline/title**"""

# Add a new column 'title_length' with the length of each title
df['title_length'] =  df['title'].str.split().apply(len)

# Display the updated DataFrame
df

"""###**Computing the embeddinsg of the title.**

**Using BERT to get the embeddings**
"""

import torch
from transformers import BertModel, BertTokenizer
from sklearn.decomposition import PCA

# Define the custom model with dimensionality reduction
class BERTEmbeddings(torch.nn.Module):
    def __init__(self, output_dim=50):
        super(BERTEmbeddings, self).__init__()
        self.bert = BertModel.from_pretrained('bert-base-uncased')  # Load pre-trained BERT model
        self.fc = torch.nn.Linear(768, output_dim)  # Linear layer to reduce from 768 to the desired dimension

    def forward(self, input_ids, attention_mask):
        # Get the embeddings from BERT
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)

        # Get the [CLS] token embedding (first token in the sequence)
        cls_embedding = outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, 768)

        # Pass the CLS embedding through the linear layer for dimensionality reduction
        reduced_embedding = self.fc(cls_embedding)  # Shape: (batch_size, output_dim)

        return reduced_embedding

# Instantiate the model
model = BERTEmbeddings(output_dim=50)  # Reduce to 50 dimensions

# Load the tokenizer for BERT
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Move the model to the appropriate device (GPU or CPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# Function to get BERT embeddings for a single title
def get_bert_embedding(title):
    inputs = tokenizer(title, return_tensors='pt', truncation=True, padding=True, max_length=128)
    inputs = {key: val.to(device) for key, val in inputs.items()}  # Move inputs to the correct device

    # Get the reduced embedding
    with torch.no_grad():
        reduced_embedding = model(inputs['input_ids'], inputs['attention_mask'])

    # Return the reduced embedding (as a numpy array)
    return reduced_embedding.cpu().numpy().flatten()  # Flatten to 1D

# Number of dimensions you want to reduce to
n_components = 2
# Apply the function to the 'title' column to get the embeddings
df['Title_embedding'] = df['title'].apply(get_bert_embedding)
# Initialize PCA
pca = PCA(n_components=n_components, random_state=42)
embedding_matrix = np.vstack(df['Title_embedding'].values)  # Shape: (num_samples, 768)
pca_embeddings = pca.fit_transform(embedding_matrix)  # Shape: (num_samples, 2)
# Create new column names for the PCA components
pca_cols = [f'pca_{i+1}' for i in range(n_components)]
df = df.drop('Title_embedding', axis=1)
# Add the PCA components to the DataFrame
df[pca_cols] = pca_embeddings

"""###**Counting the number of times each news has been shared by any users**"""

data = pd.read_csv('/content/PolitiFactNewsUser.txt',sep='\t', names=['newsid', 'user_who_shared', 'times_shared'])
# Group by 'newsid' and sum the 'times_shared' column
share_counts = data.groupby('newsid')['times_shared'].sum().reset_index()

# Rename columns for clarity before merging
# share_counts.columns = ['4', 'Total_Times_Shared']
share_counts.columns = ['News_ID', 'Total_Times_Shared']

# Merge share_counts with df on 'News_ID' (left join to keep all rows from df)
df = df.merge(share_counts, on='News_ID', how='left')

df

"""###**Counting the POS tags (Nouns, Verbs, Adjectives and Adverbs) in the news content.**"""

import nltk
from nltk import pos_tag, word_tokenize
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

def pos_features(text):
    tokens = word_tokenize(text.lower())
    tokens = [word for word in tokens if word.isalpha()]
    pos_tags = pos_tag(tokens)

    # Count POS tags
    nouns = sum(1 for word, pos in pos_tags if pos.startswith('NN'))
    verbs = sum(1 for word, pos in pos_tags if pos.startswith('VB'))
    adjectives = sum(1 for word, pos in pos_tags if pos.startswith('JJ'))
    adverbs = sum(1 for word, pos in pos_tags if pos.startswith('RB'))

    return nouns, verbs, adjectives, adverbs # Apply pos_features and create new columns in the DataFrame

df[['nouns', 'verbs', 'adjectives', 'adverbs']] = df['text'].apply(lambda x: pd.Series(pos_features(x)))

df

"""###**Counting stopwords**"""

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def count_stopwords(text):
    if isinstance(text, str):
        words = text.split()
        stopword_count = sum(1 for word in words if word.lower() in stop_words)
        return stopword_count
    return 0

df['stopwords_no'] = df['text'].apply(count_stopwords)

df

"""###**Extracting the jaccard similarity between the users sharing a particular news**"""

import networkx as nx
from sklearn.metrics import jaccard_score
from collections import defaultdict

# Load the datasets
news_user_df = pd.read_csv('/content/PolitiFactNewsUser.txt', sep='\t', header=None, names=['News_ID', 'user_id', 'frequency'], on_bad_lines='skip')
user_relationships_df = pd.read_csv('/content/PolitiFactUserUser.txt', sep='\t', header=None, names=['follower_id', 'followee_id'])

# Function to get users who shared a particular news_id
def get_users_by_news_id(news_user_df):
    result_df = news_user_df.groupby('News_ID')['user_id'].apply(list).reset_index()
    result_df['num_users'] = result_df['user_id'].apply(len)
    return result_df


# Function to get the followers for each user
def get_followers(user_id, user_relationships_df):
    followers = user_relationships_df[user_relationships_df['followee_id'] == user_id]['follower_id'].tolist()
    return set(followers)  # Return a set of followers to ensure uniqueness

# Function to calculate the Jaccard similarity between two sets (based on follower overlap)
def jaccard_similarity(set1, set2):
    intersection = len(set1.intersection(set2))
    union = len(set1.union(set2))
    return intersection / union if union != 0 else 0

# Function to calculate the average node similarity for each news_id
def calculate_average_similarity(news_user_df, user_relationships_df):
    results = []
    for news_id in news_user_df['News_ID'].unique(): # Iterate through unique news_ids
        users_sharing_news = news_user_df[news_user_df['News_ID'] == news_id]['user_id'].tolist()
        followers = {user: get_followers(user, user_relationships_df) for user in users_sharing_news}

        similarities = []
        for i in range(len(users_sharing_news)):
            for j in range(i + 1, len(users_sharing_news)):
                user1, user2 = users_sharing_news[i], users_sharing_news[j]
                similarity = jaccard_similarity(followers[user1], followers[user2])
                similarities.append(similarity)

        avg_similarity = sum(similarities) / len(similarities) if similarities else 0
        results.append({'News_ID': news_id, 'avg_similarity': avg_similarity})

    avg_similarity_df = pd.DataFrame(results)
    avg_similarity_df = avg_similarity_df.sort_values(by='News_ID').reset_index(drop=True)
    return avg_similarity_df

# Get the user list for each news_id
news_user_features_df = get_users_by_news_id(news_user_df)

# Get the average node similarity for each news_id
average_similarity_df = calculate_average_similarity(news_user_df, user_relationships_df)

df =  pd.merge(df, average_similarity_df, on='News_ID', how='left')

df

"""###**Extracting only columns that are required for the training**"""

# Select only the specified columns
my_data = df[[
    "Quote_Count",
    "title_length",
    "pca_1",
    "pca_2",
    "Total_Times_Shared",
    "nouns",
    "verbs",
    "adjectives",
    "adverbs",
    "stopwords_no",
    "avg_similarity",
    "Label"
]]
# Convert the "Label" column: 'real' to 0, 'fake' to 1
my_data["Label"] = my_data["Label"].apply(lambda x: 0 if x == "real" else 1)

my_data

import matplotlib.pyplot as plt
# Count the occurrences of each label
label_counts = my_data['Label'].value_counts().sort_index()

# Plotting
plt.figure(figsize=(6, 6))
plt.bar(['Real', 'Fake'], label_counts, color=['green', 'blue'])

# Adding labels and legend
plt.xlabel('Label')
plt.ylabel('Count')
plt.title('Distribution of Real and Fake Labels')
plt.legend(['0: Real', '1: Fake'])
plt.show()

"""**This plot shows that the number of real news represented '0' and number of fake news represented as label '1' are equal in our dataset.**"""

num_class=len(my_data['Label'].unique())
print("Total number of news labels :",num_class)

total= len(my_data)
total

"""###**Computing the baseline**"""

print('random baseline {}'.format(1.0/num_class))
print('most common baseline?')
for label in df['Label'].unique():
    print(label, len(df[df.Label==label])/total)

"""**Since the dataset has the binary labels that are equally disrtibuted, therefore both the random baseline and  most common baseline for the model is 0.5.**

##**Model Training**
"""

X = my_data.drop(columns="Label")  # Features
y = my_data["Label"]

from xgboost import XGBClassifier
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import roc_auc_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import average_precision_score

k_folds = 5
skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)
precision_list=[]
recall_list=[]
f1_list=[]
accuracy_list=[]
auc_scores = []
AVG_precision_scores = []
for fold,(tr_ind, tst_ind) in enumerate(skf.split(X, y),1):
  # Split data into training and testing sets for the current fold
  X_train, X_test = X.iloc[tr_ind], X.iloc[tst_ind]
  y_train, y_test = y.iloc[tr_ind], y.iloc[tst_ind]

  # Initialize the XGBoost classifier
  model = XGBClassifier(
        objective='binary:logistic',
        n_estimators=100,
        learning_rate=0.1,
        max_depth=6,
        eval_metric='logloss',
        random_state=42
    )
  # Train the model
  model.fit(X_train, y_train)
 # Making predictions on the test dataset frm the trained model
  y_pred = model.predict(X_test)

  proba = model.predict_proba(X_test)[:,1]

  precision = metrics.precision_score(y_test, y_pred)
  recall = metrics.recall_score(y_test, y_pred)
  f1 = metrics.f1_score(y_test, y_pred)
  accuracy = metrics.accuracy_score(y_test,y_pred)
  print(f"Confusion Matrix for fold{fold}:")
  print(metrics.confusion_matrix(y_test,y_pred))

  precision_list.append(precision)
  recall_list.append(recall)
  f1_list.append(f1)
  accuracy_list.append(accuracy)
  auc_scores.append(roc_auc_score(y_test, proba))
  AVG_precision_scores.append(average_precision_score(y_test, proba))

"""**Confusion matrix from each fold for every test set.**"""

print("precision = ", round(np.mean(precision_list)*100,3),"\n",
        "recall = ",round(np.mean(recall_list)*100,3),"\n",
        "f1 = ",round(np.mean(f1_list)*100,3),"\n",
        "accuracy = ",round(np.mean(accuracy_list)*100,3),"\n",
        "AUROC = ", round(np.mean(auc_scores)*100,3),"\n",
        "Average Precision = ", round(np.mean(AVG_precision_scores)*100,3),"\n", )

"""**The accuracy of our model is 74.16%. We can see that our model is performing better than both the random and most common baseline (50%).**"""